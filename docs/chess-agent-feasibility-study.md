# Feasibility Study - Chess Agent for Chess Learning (MCP + RAG)

## 1. Context and system objectives

The Fictional Chess Federation (FCF) aims to provide an **intelligent training assistant**, hereafter referred to as **Chess Agent**, to help players improve more efficiently  
The Chess Agent is designed to:

- Analyze chess positions
- Suggest reasonable moves
- Explain its recommendations
- Propose educational content (theoretical context, videos, related openings)

The proof of concept developed in steps 1 to 6 already includes:

- A **FastAPI backend** exposing endpoints such as `/api/v1/moves`, `/api/v1/evaluate`, `/api/v1/vector-search`, `/api/v1/videos`
- A **RAG system** based on a vector index of chess openings
- An **Angular frontend** with an interactive chessboard (`ngx-chess-board`)
- Integrations with **Lichess** and **Stockfish**

The goal of this document is to:

1. Describe the **benefits** and **limitations** of the current system
2. Propose a **target architecture** based on MCP (Model Context Protocol) to better structure Chess Agent tools
3. Evaluate the **technical feasibility** and **cost** (build and operating expenses) of moving toward production

---

## 2. Expected system features

### 2.1. End‑user perspective

- Play on an **interactive online chessboard**
- Enter or paste a **FEN** position
- Obtain:
    - A **list of suggested moves** (with numeric evaluation)
    - A **natural‑language explanation** (plans, strategic ideas) currently generated via rule-based or service-based methods; generative LLM integration is planned for future extensions
    - **Related chess openings** (via RAG)
    - Relevant **YouTube videos**
- Navigate between multiple views:
    - **Chess**: board + Chess Agent suggestions
    - **Moves & Evaluate**: direct backend API calls
    - **Vector Search (RAG)**: opening retrieval
    - **YouTube**: video content
    - **Docs**: Swagger / OpenAPI backend documentation

### 2.2. Chess Agent / orchestrator perspective

- Query several **specialized tools**:
    - Chess analysis engine (Lichess / Stockfish)
    - RAG engine for openings
    - YouTube search
    - System endpoints (health, info)
- Aggregate and rewrite results into **pedagogical natural language** using rule-based or service-based approaches; no generative LLM is currently active in the proof of concept, but LLM integration is planned as a future extension
- Maintain **session context** (current game, player objective)

### 2.3. Key technical features

- REST API documented with OpenAPI
- Optional authentication in later phases (JWT, OAuth)
- Request logging for usage analysis
- Minimal monitoring (latency, error rates)

---

## 3. Data analysis

### 3.1. Input data

- **Chess positions**:
    - FEN format (user input)
    - Optional SAN/PGN move lists (future extension)
- **Opening profiles**:
    - Short textual corpus per opening (ideas, plans, traps)
- **Video metadata**:
    - Title, description, URL, thumbnail, channel, duration
- **Technical data**:
    - API logs (timestamps, endpoints, HTTP codes)
    - RAG metrics (similarity scores, inference time)

### 3.2. Generated data

- **Embeddings** for opening descriptions
- **Move recommendations** (move + score + commentary)
- **Natural‑language explanations** generated by the Chess Agent (currently rule-based or service-based)
- **Usage statistics** (request count, latency, etc.)

### 3.3. Data constraints

- Initially small data volume (dozens of openings), expected to grow
- Need for **consistency** between:
    - Engine evaluation (Stockfish)
    - Chess Agent explanations
    - RAG‑retrieved openings
- Moderate data sensitivity (limited GDPR constraints)

---

## 4. Technical architecture (core deliverable with MCP)

### 4.1. Role of MCP in the architecture

MCP (Model Context Protocol) is proposed to **standardize communication** between the Chess Agent (MCP client) and domain‑specific services designed to be exposed as **MCP servers** in a target architecture:

- MCP **Chess Engine** server (moves / evaluate)
- MCP **RAG / Vector Search** server
- MCP **YouTube Search** server
- MCP **System / Monitoring** server

This approach is intended to allow:

- Clear **separation** between model reasoning and business logic
- **Tool or LLM provider changes** without impacting the Angular frontend
- Improved **observability**, with each tool monitored independently

The current proof of concept relies on REST APIs for backend communication; MCP is a planned evolution for better modularity and orchestration.

### 4.2. High‑level overview (Mermaid)

The system architecture diagram is maintained separately. Both the Mermaid source and the rendered SVG are available in the same `docs/` folder.

- [Architecture diagram (Mermaid source)](diagram.mmd)
- [Architecture diagram (SVG render)](diagram.svg)

### 4.3. Main components

- **Angular frontend**
    - Application: `agent-ia-frontend`
    - Routes: `/`, `/play`, `/analysis`, `/openings`, `/videos`
    - `HomeChessComponent` based on `ngx-chess-board` v3.0.0
    - Angular services for backend communication
- **FastAPI backend**
    - REST endpoints documented with OpenAPI
    - Exposes `/api/v1/moves`, `/api/v1/evaluate`, `/api/v1/vector-search`, `/api/v1/videos`, `/api/v1/health`
    - Acts as a lightweight API gateway
    - CORS handling for Angular access
- **Chess Agent (orchestration logic + MCP client)**
    - Encapsulates prompts and conversational logic
    - Selects MCP tools based on user intent
    - Aggregates tool responses into pedagogical output using rule-based or service-based methods; no generative LLM is currently active in the proof of concept, but LLM integration is planned as a future extension
- **MCP Servers**
    - `Chess Engine Server`: Stockfish/Lichess analysis
    - `Chess Openings RAG Server`: Milvus vector search
    - `YouTube Search Server`: YouTube API integration
    - `System Monitoring Server`: health and metrics
- **Storage**
    - Milvus: vector embeddings
    - MongoDB (planned): logs, metadata, future user preferences

---

## 5. Recommended technologies

- **Backend**
    - Python 3.x, FastAPI, Uvicorn / Gunicorn
    - HTTPX / Requests for external calls
- **AI / RAG**
    - `sentence-transformers` for embeddings
    - Milvus (or Qdrant, Chroma) for vector indexing
    - LangChain or equivalent for Chess Agent orchestration
- **MCP**
    - Official MCP client/server implementation
    - Each tool packaged as an independent, scalable service
- **Frontend**
    - Angular 17
    - `ngx-chess-board` v3.0.0 (locally built from maintainer GitLab)
    - TypeScript + RxJS
- **Infrastructure / DevOps**
    - Docker / Docker Compose for the POC
    - Future option: Kubernetes
    - CI/CD via GitLab CI

---

## 6. Evaluation methodology

### 6.1. Functional criteria

- **Move accuracy**
    - Compare Chess Agent suggestions with Stockfish best lines
- **RAG relevance**
    - Validate opening matches (top‑1 / top‑3 accuracy)
- **Explanation quality**
    - Qualitative evaluation by players of different levels

### 6.2. Technical criteria

- **Performance**
    - `/api/v1/moves`: < 1 s
    - `/api/v1/evaluate`: < 2-3 s
    - `/api/v1/vector-search`: < 500 ms
- **Robustness**
    - Graceful handling of API errors and empty RAG results
- **Scalability**
    - Load testing with concurrent users

### 6.3. UX criteria

- Simple UI, minimal inputs
- Clear feedback for loading and error states
- Readable presentation of moves, explanations, and videos

---

## 7. Limitations and constraints

### 7.1. Technical limitations

- Dependence on external APIs (Lichess, YouTube, LLM providers)
- Limited initial opening corpus
- CPU‑intensive Stockfish analysis
- MCP standard still evolving; current POC does not yet implement MCP fully

### 7.2. Organizational constraints

- Required skills:
    - Python / FastAPI
    - Angular
    - Basic MLOps / RAG
    - Chess domain knowledge
- Coordination between AI, frontend, and infra teams

---

## 8. Cost and resource estimates

### 8.1. Build costs

- Backend development: 20-30 person‑days
- Frontend development: 10-15 person‑days
- AI / RAG work: 15-20 person‑days
- Integration & testing: 5-10 person‑days
- Documentation: 3-5 person‑days

Estimated build cost: **€25k-€40k**

### 8.2. Operating costs (monthly)

- Infrastructure: €150-350
- LLM & APIs: €30-150 (usage‑dependent)
- Total estimated OPEX: **€200-€500 / month**

---

## 9. Conclusion

The current proof of concept demonstrates that it is technically feasible to build a **Chess Agent** capable of:

- Combining chess engine analysis, RAG‑based opening knowledge, and video content
- Exposing these capabilities through a FastAPI backend and an Angular frontend
- Structuring tools cleanly using MCP as a target architecture

The solution is suitable as a foundation for a club‑or federation‑level pilot and can evolve toward a production‑ready chess training platform

Future work may include:

- Expanding the opening and endgame corpus
- Adding new MCP tools (PGN analysis, training plans, quizzes)
- Improving observability and prompt/model experimentation

---
